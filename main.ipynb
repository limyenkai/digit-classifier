{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 1),\n",
    "\n",
    "    'test': DataLoader(test_data,\n",
    "                       batch_size = 100,\n",
    "                       shuffle = True,\n",
    "                       num_workers = 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()               # zero out gradient\n",
    "        output = model(data)                # current state prediction\n",
    "        loss = loss_fn(output, target)      # calculate loss\n",
    "        loss.backward()                     # back propagate\n",
    "        optimizer.step()                    # do optimization step\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim = 1, keepdim = True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Acuracy {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%\\n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/p045mwns5lb5bvtrs9y0wsmr0000gn/T/ipykernel_381/3258896293.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.303049\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t2.291970\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t2.172651\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t1.949526\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t1.935638\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t1.796543\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t1.846325\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t1.716427\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.785925\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t1.717030\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t1.677246\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t1.657067\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t1.754367\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t1.681199\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t1.595002\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t1.643843\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t1.692190\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t1.631001\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t1.647801\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t1.599697\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t1.636879\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t1.585713\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t1.578364\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t1.580728\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t1.611168\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t1.562215\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t1.582877\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t1.646774\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t1.661930\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t1.596603\n",
      "\n",
      "Test set: Average loss: 0.0153, Acuracy 9366/10000 (94%\n",
      ")\n",
      "Train Epoch: 2 [0/60000 (0%)]\t1.646610\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t1.577555\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t1.685074\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t1.561098\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t1.612947\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t1.616494\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t1.590208\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t1.574039\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t1.634063\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t1.555530\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t1.624371\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t1.529126\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t1.611431\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t1.586503\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t1.606892\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t1.542794\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t1.577538\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t1.631701\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t1.553799\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t1.574789\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t1.530148\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t1.600698\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t1.551196\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t1.624175\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t1.541208\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t1.582818\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t1.540808\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t1.564703\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t1.533522\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t1.558990\n",
      "\n",
      "Test set: Average loss: 0.0151, Acuracy 9500/10000 (95%\n",
      ")\n",
      "Train Epoch: 3 [0/60000 (0%)]\t1.573679\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t1.603577\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t1.550041\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t1.616868\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t1.518339\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t1.513277\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t1.596962\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t1.555938\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t1.593498\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t1.550454\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t1.534430\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t1.578344\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t1.567338\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t1.569951\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t1.593879\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t1.527621\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t1.626268\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t1.531013\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t1.562891\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t1.530842\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t1.563570\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t1.527538\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t1.548660\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t1.603904\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t1.572497\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t1.561322\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t1.561988\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t1.550996\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t1.503649\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t1.544114\n",
      "\n",
      "Test set: Average loss: 0.0150, Acuracy 9574/10000 (96%\n",
      ")\n",
      "Train Epoch: 4 [0/60000 (0%)]\t1.573878\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t1.519945\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t1.515949\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t1.570758\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t1.567758\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t1.596036\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t1.573171\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t1.532480\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t1.581740\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t1.537453\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t1.606912\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t1.540004\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t1.573072\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t1.548231\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t1.522599\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t1.561512\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t1.556206\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t1.630173\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t1.605963\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t1.584764\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t1.584165\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t1.527458\n",
      "Train Epoch: 4 [44000/60000 (73%)]\t1.552191\n",
      "Train Epoch: 4 [46000/60000 (77%)]\t1.516784\n",
      "Train Epoch: 4 [48000/60000 (80%)]\t1.550200\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t1.565760\n",
      "Train Epoch: 4 [52000/60000 (87%)]\t1.531151\n",
      "Train Epoch: 4 [54000/60000 (90%)]\t1.578610\n",
      "Train Epoch: 4 [56000/60000 (93%)]\t1.571407\n",
      "Train Epoch: 4 [58000/60000 (97%)]\t1.579994\n",
      "\n",
      "Test set: Average loss: 0.0150, Acuracy 9627/10000 (96%\n",
      ")\n",
      "Train Epoch: 5 [0/60000 (0%)]\t1.545821\n",
      "Train Epoch: 5 [2000/60000 (3%)]\t1.534716\n",
      "Train Epoch: 5 [4000/60000 (7%)]\t1.529538\n",
      "Train Epoch: 5 [6000/60000 (10%)]\t1.522465\n",
      "Train Epoch: 5 [8000/60000 (13%)]\t1.519002\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t1.541170\n",
      "Train Epoch: 5 [12000/60000 (20%)]\t1.573487\n",
      "Train Epoch: 5 [14000/60000 (23%)]\t1.558562\n",
      "Train Epoch: 5 [16000/60000 (27%)]\t1.589925\n",
      "Train Epoch: 5 [18000/60000 (30%)]\t1.539567\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t1.546837\n",
      "Train Epoch: 5 [22000/60000 (37%)]\t1.553926\n",
      "Train Epoch: 5 [24000/60000 (40%)]\t1.573230\n",
      "Train Epoch: 5 [26000/60000 (43%)]\t1.536604\n",
      "Train Epoch: 5 [28000/60000 (47%)]\t1.529123\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t1.551163\n",
      "Train Epoch: 5 [32000/60000 (53%)]\t1.502193\n",
      "Train Epoch: 5 [34000/60000 (57%)]\t1.546338\n",
      "Train Epoch: 5 [36000/60000 (60%)]\t1.533983\n",
      "Train Epoch: 5 [38000/60000 (63%)]\t1.560765\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t1.541716\n",
      "Train Epoch: 5 [42000/60000 (70%)]\t1.511097\n",
      "Train Epoch: 5 [44000/60000 (73%)]\t1.507238\n",
      "Train Epoch: 5 [46000/60000 (77%)]\t1.556477\n",
      "Train Epoch: 5 [48000/60000 (80%)]\t1.597166\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t1.569996\n",
      "Train Epoch: 5 [52000/60000 (87%)]\t1.562733\n",
      "Train Epoch: 5 [54000/60000 (90%)]\t1.546951\n",
      "Train Epoch: 5 [56000/60000 (93%)]\t1.552381\n",
      "Train Epoch: 5 [58000/60000 (97%)]\t1.512789\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9670/10000 (97%\n",
      ")\n",
      "Train Epoch: 6 [0/60000 (0%)]\t1.533317\n",
      "Train Epoch: 6 [2000/60000 (3%)]\t1.608565\n",
      "Train Epoch: 6 [4000/60000 (7%)]\t1.563556\n",
      "Train Epoch: 6 [6000/60000 (10%)]\t1.579326\n",
      "Train Epoch: 6 [8000/60000 (13%)]\t1.519051\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t1.544151\n",
      "Train Epoch: 6 [12000/60000 (20%)]\t1.556139\n",
      "Train Epoch: 6 [14000/60000 (23%)]\t1.591268\n",
      "Train Epoch: 6 [16000/60000 (27%)]\t1.549739\n",
      "Train Epoch: 6 [18000/60000 (30%)]\t1.489727\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t1.513026\n",
      "Train Epoch: 6 [22000/60000 (37%)]\t1.507754\n",
      "Train Epoch: 6 [24000/60000 (40%)]\t1.513297\n",
      "Train Epoch: 6 [26000/60000 (43%)]\t1.503828\n",
      "Train Epoch: 6 [28000/60000 (47%)]\t1.546844\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t1.514427\n",
      "Train Epoch: 6 [32000/60000 (53%)]\t1.515233\n",
      "Train Epoch: 6 [34000/60000 (57%)]\t1.606692\n",
      "Train Epoch: 6 [36000/60000 (60%)]\t1.549200\n",
      "Train Epoch: 6 [38000/60000 (63%)]\t1.567434\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t1.538117\n",
      "Train Epoch: 6 [42000/60000 (70%)]\t1.512794\n",
      "Train Epoch: 6 [44000/60000 (73%)]\t1.532750\n",
      "Train Epoch: 6 [46000/60000 (77%)]\t1.561870\n",
      "Train Epoch: 6 [48000/60000 (80%)]\t1.538858\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t1.525102\n",
      "Train Epoch: 6 [52000/60000 (87%)]\t1.516387\n",
      "Train Epoch: 6 [54000/60000 (90%)]\t1.505987\n",
      "Train Epoch: 6 [56000/60000 (93%)]\t1.537359\n",
      "Train Epoch: 6 [58000/60000 (97%)]\t1.532045\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9669/10000 (97%\n",
      ")\n",
      "Train Epoch: 7 [0/60000 (0%)]\t1.530066\n",
      "Train Epoch: 7 [2000/60000 (3%)]\t1.572320\n",
      "Train Epoch: 7 [4000/60000 (7%)]\t1.596396\n",
      "Train Epoch: 7 [6000/60000 (10%)]\t1.509179\n",
      "Train Epoch: 7 [8000/60000 (13%)]\t1.494366\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t1.536744\n",
      "Train Epoch: 7 [12000/60000 (20%)]\t1.517774\n",
      "Train Epoch: 7 [14000/60000 (23%)]\t1.527164\n",
      "Train Epoch: 7 [16000/60000 (27%)]\t1.525644\n",
      "Train Epoch: 7 [18000/60000 (30%)]\t1.553690\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t1.547688\n",
      "Train Epoch: 7 [22000/60000 (37%)]\t1.544882\n",
      "Train Epoch: 7 [24000/60000 (40%)]\t1.511076\n",
      "Train Epoch: 7 [26000/60000 (43%)]\t1.527872\n",
      "Train Epoch: 7 [28000/60000 (47%)]\t1.511371\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t1.555802\n",
      "Train Epoch: 7 [32000/60000 (53%)]\t1.563683\n",
      "Train Epoch: 7 [34000/60000 (57%)]\t1.521207\n",
      "Train Epoch: 7 [36000/60000 (60%)]\t1.516034\n",
      "Train Epoch: 7 [38000/60000 (63%)]\t1.544839\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t1.527482\n",
      "Train Epoch: 7 [42000/60000 (70%)]\t1.552966\n",
      "Train Epoch: 7 [44000/60000 (73%)]\t1.592773\n",
      "Train Epoch: 7 [46000/60000 (77%)]\t1.592414\n",
      "Train Epoch: 7 [48000/60000 (80%)]\t1.577061\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t1.538418\n",
      "Train Epoch: 7 [52000/60000 (87%)]\t1.555952\n",
      "Train Epoch: 7 [54000/60000 (90%)]\t1.522036\n",
      "Train Epoch: 7 [56000/60000 (93%)]\t1.524095\n",
      "Train Epoch: 7 [58000/60000 (97%)]\t1.556619\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9689/10000 (97%\n",
      ")\n",
      "Train Epoch: 8 [0/60000 (0%)]\t1.517474\n",
      "Train Epoch: 8 [2000/60000 (3%)]\t1.567575\n",
      "Train Epoch: 8 [4000/60000 (7%)]\t1.539768\n",
      "Train Epoch: 8 [6000/60000 (10%)]\t1.531741\n",
      "Train Epoch: 8 [8000/60000 (13%)]\t1.554623\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t1.550719\n",
      "Train Epoch: 8 [12000/60000 (20%)]\t1.553684\n",
      "Train Epoch: 8 [14000/60000 (23%)]\t1.550046\n",
      "Train Epoch: 8 [16000/60000 (27%)]\t1.557610\n",
      "Train Epoch: 8 [18000/60000 (30%)]\t1.503875\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t1.547621\n",
      "Train Epoch: 8 [22000/60000 (37%)]\t1.553969\n",
      "Train Epoch: 8 [24000/60000 (40%)]\t1.524960\n",
      "Train Epoch: 8 [26000/60000 (43%)]\t1.534421\n",
      "Train Epoch: 8 [28000/60000 (47%)]\t1.523944\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t1.470281\n",
      "Train Epoch: 8 [32000/60000 (53%)]\t1.534579\n",
      "Train Epoch: 8 [34000/60000 (57%)]\t1.511123\n",
      "Train Epoch: 8 [36000/60000 (60%)]\t1.538266\n",
      "Train Epoch: 8 [38000/60000 (63%)]\t1.555071\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t1.504425\n",
      "Train Epoch: 8 [42000/60000 (70%)]\t1.524199\n",
      "Train Epoch: 8 [44000/60000 (73%)]\t1.497010\n",
      "Train Epoch: 8 [46000/60000 (77%)]\t1.530642\n",
      "Train Epoch: 8 [48000/60000 (80%)]\t1.525897\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t1.544718\n",
      "Train Epoch: 8 [52000/60000 (87%)]\t1.504388\n",
      "Train Epoch: 8 [54000/60000 (90%)]\t1.535661\n",
      "Train Epoch: 8 [56000/60000 (93%)]\t1.523059\n",
      "Train Epoch: 8 [58000/60000 (97%)]\t1.520123\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9707/10000 (97%\n",
      ")\n",
      "Train Epoch: 9 [0/60000 (0%)]\t1.522223\n",
      "Train Epoch: 9 [2000/60000 (3%)]\t1.581228\n",
      "Train Epoch: 9 [4000/60000 (7%)]\t1.520741\n",
      "Train Epoch: 9 [6000/60000 (10%)]\t1.534880\n",
      "Train Epoch: 9 [8000/60000 (13%)]\t1.487226\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t1.517124\n",
      "Train Epoch: 9 [12000/60000 (20%)]\t1.541838\n",
      "Train Epoch: 9 [14000/60000 (23%)]\t1.505875\n",
      "Train Epoch: 9 [16000/60000 (27%)]\t1.499621\n",
      "Train Epoch: 9 [18000/60000 (30%)]\t1.542893\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t1.575033\n",
      "Train Epoch: 9 [22000/60000 (37%)]\t1.532941\n",
      "Train Epoch: 9 [24000/60000 (40%)]\t1.510324\n",
      "Train Epoch: 9 [26000/60000 (43%)]\t1.510177\n",
      "Train Epoch: 9 [28000/60000 (47%)]\t1.500701\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t1.507767\n",
      "Train Epoch: 9 [32000/60000 (53%)]\t1.555674\n",
      "Train Epoch: 9 [34000/60000 (57%)]\t1.518493\n",
      "Train Epoch: 9 [36000/60000 (60%)]\t1.485428\n",
      "Train Epoch: 9 [38000/60000 (63%)]\t1.568632\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t1.525479\n",
      "Train Epoch: 9 [42000/60000 (70%)]\t1.531707\n",
      "Train Epoch: 9 [44000/60000 (73%)]\t1.519161\n",
      "Train Epoch: 9 [46000/60000 (77%)]\t1.529796\n",
      "Train Epoch: 9 [48000/60000 (80%)]\t1.525552\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t1.569914\n",
      "Train Epoch: 9 [52000/60000 (87%)]\t1.554719\n",
      "Train Epoch: 9 [54000/60000 (90%)]\t1.515046\n",
      "Train Epoch: 9 [56000/60000 (93%)]\t1.506869\n",
      "Train Epoch: 9 [58000/60000 (97%)]\t1.519537\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9722/10000 (97%\n",
      ")\n",
      "Train Epoch: 10 [0/60000 (0%)]\t1.496408\n",
      "Train Epoch: 10 [2000/60000 (3%)]\t1.537613\n",
      "Train Epoch: 10 [4000/60000 (7%)]\t1.518563\n",
      "Train Epoch: 10 [6000/60000 (10%)]\t1.533749\n",
      "Train Epoch: 10 [8000/60000 (13%)]\t1.522465\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t1.531994\n",
      "Train Epoch: 10 [12000/60000 (20%)]\t1.505209\n",
      "Train Epoch: 10 [14000/60000 (23%)]\t1.530631\n",
      "Train Epoch: 10 [16000/60000 (27%)]\t1.505506\n",
      "Train Epoch: 10 [18000/60000 (30%)]\t1.514741\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t1.521405\n",
      "Train Epoch: 10 [22000/60000 (37%)]\t1.482514\n",
      "Train Epoch: 10 [24000/60000 (40%)]\t1.524278\n",
      "Train Epoch: 10 [26000/60000 (43%)]\t1.503792\n",
      "Train Epoch: 10 [28000/60000 (47%)]\t1.493028\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t1.509295\n",
      "Train Epoch: 10 [32000/60000 (53%)]\t1.485545\n",
      "Train Epoch: 10 [34000/60000 (57%)]\t1.494776\n",
      "Train Epoch: 10 [36000/60000 (60%)]\t1.551974\n",
      "Train Epoch: 10 [38000/60000 (63%)]\t1.541903\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t1.507719\n",
      "Train Epoch: 10 [42000/60000 (70%)]\t1.533607\n",
      "Train Epoch: 10 [44000/60000 (73%)]\t1.543428\n",
      "Train Epoch: 10 [46000/60000 (77%)]\t1.513039\n",
      "Train Epoch: 10 [48000/60000 (80%)]\t1.500234\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t1.571350\n",
      "Train Epoch: 10 [52000/60000 (87%)]\t1.543113\n",
      "Train Epoch: 10 [54000/60000 (90%)]\t1.503504\n",
      "Train Epoch: 10 [56000/60000 (93%)]\t1.512446\n",
      "Train Epoch: 10 [58000/60000 (97%)]\t1.543707\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9718/10000 (97%\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/p045mwns5lb5bvtrs9y0wsmr0000gn/T/ipykernel_381/3258896293.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[4]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim = 1, keepdim = True).item()\n",
    "\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
