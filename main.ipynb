{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(train_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle = True,\n",
    "                        num_workers = 1),\n",
    "\n",
    "    'test': DataLoader(test_data,\n",
    "                       batch_size = 100,\n",
    "                       shuffle = True,\n",
    "                       num_workers = 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()               # zero out gradient\n",
    "        output = model(data)                # current state prediction\n",
    "        loss = loss_fn(output, target)      # calculate loss\n",
    "        loss.backward()                     # back propagate\n",
    "        optimizer.step()                    # do optimization step\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6f}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim = 1, keepdim = True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Acuracy {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct / len(loaders[\"test\"].dataset):.0f}%\\n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/p045mwns5lb5bvtrs9y0wsmr0000gn/T/ipykernel_381/3258896293.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t2.303049\n",
      "Train Epoch: 1 [2000/60000 (3%)]\t2.291970\n",
      "Train Epoch: 1 [4000/60000 (7%)]\t2.172651\n",
      "Train Epoch: 1 [6000/60000 (10%)]\t1.949526\n",
      "Train Epoch: 1 [8000/60000 (13%)]\t1.935638\n",
      "Train Epoch: 1 [10000/60000 (17%)]\t1.796543\n",
      "Train Epoch: 1 [12000/60000 (20%)]\t1.846325\n",
      "Train Epoch: 1 [14000/60000 (23%)]\t1.716427\n",
      "Train Epoch: 1 [16000/60000 (27%)]\t1.785925\n",
      "Train Epoch: 1 [18000/60000 (30%)]\t1.717030\n",
      "Train Epoch: 1 [20000/60000 (33%)]\t1.677246\n",
      "Train Epoch: 1 [22000/60000 (37%)]\t1.657067\n",
      "Train Epoch: 1 [24000/60000 (40%)]\t1.754367\n",
      "Train Epoch: 1 [26000/60000 (43%)]\t1.681199\n",
      "Train Epoch: 1 [28000/60000 (47%)]\t1.595002\n",
      "Train Epoch: 1 [30000/60000 (50%)]\t1.643843\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t1.692190\n",
      "Train Epoch: 1 [34000/60000 (57%)]\t1.631001\n",
      "Train Epoch: 1 [36000/60000 (60%)]\t1.647801\n",
      "Train Epoch: 1 [38000/60000 (63%)]\t1.599697\n",
      "Train Epoch: 1 [40000/60000 (67%)]\t1.636879\n",
      "Train Epoch: 1 [42000/60000 (70%)]\t1.585713\n",
      "Train Epoch: 1 [44000/60000 (73%)]\t1.578364\n",
      "Train Epoch: 1 [46000/60000 (77%)]\t1.580728\n",
      "Train Epoch: 1 [48000/60000 (80%)]\t1.611168\n",
      "Train Epoch: 1 [50000/60000 (83%)]\t1.562215\n",
      "Train Epoch: 1 [52000/60000 (87%)]\t1.582877\n",
      "Train Epoch: 1 [54000/60000 (90%)]\t1.646774\n",
      "Train Epoch: 1 [56000/60000 (93%)]\t1.661930\n",
      "Train Epoch: 1 [58000/60000 (97%)]\t1.596603\n",
      "\n",
      "Test set: Average loss: 0.0153, Acuracy 9366/10000 (94%\n",
      ")\n",
      "Train Epoch: 2 [0/60000 (0%)]\t1.646610\n",
      "Train Epoch: 2 [2000/60000 (3%)]\t1.577555\n",
      "Train Epoch: 2 [4000/60000 (7%)]\t1.685074\n",
      "Train Epoch: 2 [6000/60000 (10%)]\t1.561098\n",
      "Train Epoch: 2 [8000/60000 (13%)]\t1.612947\n",
      "Train Epoch: 2 [10000/60000 (17%)]\t1.616494\n",
      "Train Epoch: 2 [12000/60000 (20%)]\t1.590208\n",
      "Train Epoch: 2 [14000/60000 (23%)]\t1.574039\n",
      "Train Epoch: 2 [16000/60000 (27%)]\t1.634063\n",
      "Train Epoch: 2 [18000/60000 (30%)]\t1.555530\n",
      "Train Epoch: 2 [20000/60000 (33%)]\t1.624371\n",
      "Train Epoch: 2 [22000/60000 (37%)]\t1.529126\n",
      "Train Epoch: 2 [24000/60000 (40%)]\t1.611431\n",
      "Train Epoch: 2 [26000/60000 (43%)]\t1.586503\n",
      "Train Epoch: 2 [28000/60000 (47%)]\t1.606892\n",
      "Train Epoch: 2 [30000/60000 (50%)]\t1.542794\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t1.577538\n",
      "Train Epoch: 2 [34000/60000 (57%)]\t1.631701\n",
      "Train Epoch: 2 [36000/60000 (60%)]\t1.553799\n",
      "Train Epoch: 2 [38000/60000 (63%)]\t1.574789\n",
      "Train Epoch: 2 [40000/60000 (67%)]\t1.530148\n",
      "Train Epoch: 2 [42000/60000 (70%)]\t1.600698\n",
      "Train Epoch: 2 [44000/60000 (73%)]\t1.551196\n",
      "Train Epoch: 2 [46000/60000 (77%)]\t1.624175\n",
      "Train Epoch: 2 [48000/60000 (80%)]\t1.541208\n",
      "Train Epoch: 2 [50000/60000 (83%)]\t1.582818\n",
      "Train Epoch: 2 [52000/60000 (87%)]\t1.540808\n",
      "Train Epoch: 2 [54000/60000 (90%)]\t1.564703\n",
      "Train Epoch: 2 [56000/60000 (93%)]\t1.533522\n",
      "Train Epoch: 2 [58000/60000 (97%)]\t1.558990\n",
      "\n",
      "Test set: Average loss: 0.0151, Acuracy 9500/10000 (95%\n",
      ")\n",
      "Train Epoch: 3 [0/60000 (0%)]\t1.573679\n",
      "Train Epoch: 3 [2000/60000 (3%)]\t1.603577\n",
      "Train Epoch: 3 [4000/60000 (7%)]\t1.550041\n",
      "Train Epoch: 3 [6000/60000 (10%)]\t1.616868\n",
      "Train Epoch: 3 [8000/60000 (13%)]\t1.518339\n",
      "Train Epoch: 3 [10000/60000 (17%)]\t1.513277\n",
      "Train Epoch: 3 [12000/60000 (20%)]\t1.596962\n",
      "Train Epoch: 3 [14000/60000 (23%)]\t1.555938\n",
      "Train Epoch: 3 [16000/60000 (27%)]\t1.593498\n",
      "Train Epoch: 3 [18000/60000 (30%)]\t1.550454\n",
      "Train Epoch: 3 [20000/60000 (33%)]\t1.534430\n",
      "Train Epoch: 3 [22000/60000 (37%)]\t1.578344\n",
      "Train Epoch: 3 [24000/60000 (40%)]\t1.567338\n",
      "Train Epoch: 3 [26000/60000 (43%)]\t1.569951\n",
      "Train Epoch: 3 [28000/60000 (47%)]\t1.593879\n",
      "Train Epoch: 3 [30000/60000 (50%)]\t1.527621\n",
      "Train Epoch: 3 [32000/60000 (53%)]\t1.626268\n",
      "Train Epoch: 3 [34000/60000 (57%)]\t1.531013\n",
      "Train Epoch: 3 [36000/60000 (60%)]\t1.562891\n",
      "Train Epoch: 3 [38000/60000 (63%)]\t1.530842\n",
      "Train Epoch: 3 [40000/60000 (67%)]\t1.563570\n",
      "Train Epoch: 3 [42000/60000 (70%)]\t1.527538\n",
      "Train Epoch: 3 [44000/60000 (73%)]\t1.548660\n",
      "Train Epoch: 3 [46000/60000 (77%)]\t1.603904\n",
      "Train Epoch: 3 [48000/60000 (80%)]\t1.572497\n",
      "Train Epoch: 3 [50000/60000 (83%)]\t1.561322\n",
      "Train Epoch: 3 [52000/60000 (87%)]\t1.561988\n",
      "Train Epoch: 3 [54000/60000 (90%)]\t1.550996\n",
      "Train Epoch: 3 [56000/60000 (93%)]\t1.503649\n",
      "Train Epoch: 3 [58000/60000 (97%)]\t1.544114\n",
      "\n",
      "Test set: Average loss: 0.0150, Acuracy 9574/10000 (96%\n",
      ")\n",
      "Train Epoch: 4 [0/60000 (0%)]\t1.573878\n",
      "Train Epoch: 4 [2000/60000 (3%)]\t1.519945\n",
      "Train Epoch: 4 [4000/60000 (7%)]\t1.515949\n",
      "Train Epoch: 4 [6000/60000 (10%)]\t1.570758\n",
      "Train Epoch: 4 [8000/60000 (13%)]\t1.567758\n",
      "Train Epoch: 4 [10000/60000 (17%)]\t1.596036\n",
      "Train Epoch: 4 [12000/60000 (20%)]\t1.573171\n",
      "Train Epoch: 4 [14000/60000 (23%)]\t1.532480\n",
      "Train Epoch: 4 [16000/60000 (27%)]\t1.581740\n",
      "Train Epoch: 4 [18000/60000 (30%)]\t1.537453\n",
      "Train Epoch: 4 [20000/60000 (33%)]\t1.606912\n",
      "Train Epoch: 4 [22000/60000 (37%)]\t1.540004\n",
      "Train Epoch: 4 [24000/60000 (40%)]\t1.573072\n",
      "Train Epoch: 4 [26000/60000 (43%)]\t1.548231\n",
      "Train Epoch: 4 [28000/60000 (47%)]\t1.522599\n",
      "Train Epoch: 4 [30000/60000 (50%)]\t1.561512\n",
      "Train Epoch: 4 [32000/60000 (53%)]\t1.556206\n",
      "Train Epoch: 4 [34000/60000 (57%)]\t1.630173\n",
      "Train Epoch: 4 [36000/60000 (60%)]\t1.605963\n",
      "Train Epoch: 4 [38000/60000 (63%)]\t1.584764\n",
      "Train Epoch: 4 [40000/60000 (67%)]\t1.584165\n",
      "Train Epoch: 4 [42000/60000 (70%)]\t1.527458\n",
      "Train Epoch: 4 [44000/60000 (73%)]\t1.552191\n",
      "Train Epoch: 4 [46000/60000 (77%)]\t1.516784\n",
      "Train Epoch: 4 [48000/60000 (80%)]\t1.550200\n",
      "Train Epoch: 4 [50000/60000 (83%)]\t1.565760\n",
      "Train Epoch: 4 [52000/60000 (87%)]\t1.531151\n",
      "Train Epoch: 4 [54000/60000 (90%)]\t1.578610\n",
      "Train Epoch: 4 [56000/60000 (93%)]\t1.571407\n",
      "Train Epoch: 4 [58000/60000 (97%)]\t1.579994\n",
      "\n",
      "Test set: Average loss: 0.0150, Acuracy 9627/10000 (96%\n",
      ")\n",
      "Train Epoch: 5 [0/60000 (0%)]\t1.545821\n",
      "Train Epoch: 5 [2000/60000 (3%)]\t1.534716\n",
      "Train Epoch: 5 [4000/60000 (7%)]\t1.529538\n",
      "Train Epoch: 5 [6000/60000 (10%)]\t1.522465\n",
      "Train Epoch: 5 [8000/60000 (13%)]\t1.519002\n",
      "Train Epoch: 5 [10000/60000 (17%)]\t1.541170\n",
      "Train Epoch: 5 [12000/60000 (20%)]\t1.573487\n",
      "Train Epoch: 5 [14000/60000 (23%)]\t1.558562\n",
      "Train Epoch: 5 [16000/60000 (27%)]\t1.589925\n",
      "Train Epoch: 5 [18000/60000 (30%)]\t1.539567\n",
      "Train Epoch: 5 [20000/60000 (33%)]\t1.546837\n",
      "Train Epoch: 5 [22000/60000 (37%)]\t1.553926\n",
      "Train Epoch: 5 [24000/60000 (40%)]\t1.573230\n",
      "Train Epoch: 5 [26000/60000 (43%)]\t1.536604\n",
      "Train Epoch: 5 [28000/60000 (47%)]\t1.529123\n",
      "Train Epoch: 5 [30000/60000 (50%)]\t1.551163\n",
      "Train Epoch: 5 [32000/60000 (53%)]\t1.502193\n",
      "Train Epoch: 5 [34000/60000 (57%)]\t1.546338\n",
      "Train Epoch: 5 [36000/60000 (60%)]\t1.533983\n",
      "Train Epoch: 5 [38000/60000 (63%)]\t1.560765\n",
      "Train Epoch: 5 [40000/60000 (67%)]\t1.541716\n",
      "Train Epoch: 5 [42000/60000 (70%)]\t1.511097\n",
      "Train Epoch: 5 [44000/60000 (73%)]\t1.507238\n",
      "Train Epoch: 5 [46000/60000 (77%)]\t1.556477\n",
      "Train Epoch: 5 [48000/60000 (80%)]\t1.597166\n",
      "Train Epoch: 5 [50000/60000 (83%)]\t1.569996\n",
      "Train Epoch: 5 [52000/60000 (87%)]\t1.562733\n",
      "Train Epoch: 5 [54000/60000 (90%)]\t1.546951\n",
      "Train Epoch: 5 [56000/60000 (93%)]\t1.552381\n",
      "Train Epoch: 5 [58000/60000 (97%)]\t1.512789\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9670/10000 (97%\n",
      ")\n",
      "Train Epoch: 6 [0/60000 (0%)]\t1.533317\n",
      "Train Epoch: 6 [2000/60000 (3%)]\t1.608565\n",
      "Train Epoch: 6 [4000/60000 (7%)]\t1.563556\n",
      "Train Epoch: 6 [6000/60000 (10%)]\t1.579326\n",
      "Train Epoch: 6 [8000/60000 (13%)]\t1.519051\n",
      "Train Epoch: 6 [10000/60000 (17%)]\t1.544151\n",
      "Train Epoch: 6 [12000/60000 (20%)]\t1.556139\n",
      "Train Epoch: 6 [14000/60000 (23%)]\t1.591268\n",
      "Train Epoch: 6 [16000/60000 (27%)]\t1.549739\n",
      "Train Epoch: 6 [18000/60000 (30%)]\t1.489727\n",
      "Train Epoch: 6 [20000/60000 (33%)]\t1.513026\n",
      "Train Epoch: 6 [22000/60000 (37%)]\t1.507754\n",
      "Train Epoch: 6 [24000/60000 (40%)]\t1.513297\n",
      "Train Epoch: 6 [26000/60000 (43%)]\t1.503828\n",
      "Train Epoch: 6 [28000/60000 (47%)]\t1.546844\n",
      "Train Epoch: 6 [30000/60000 (50%)]\t1.514427\n",
      "Train Epoch: 6 [32000/60000 (53%)]\t1.515233\n",
      "Train Epoch: 6 [34000/60000 (57%)]\t1.606692\n",
      "Train Epoch: 6 [36000/60000 (60%)]\t1.549200\n",
      "Train Epoch: 6 [38000/60000 (63%)]\t1.567434\n",
      "Train Epoch: 6 [40000/60000 (67%)]\t1.538117\n",
      "Train Epoch: 6 [42000/60000 (70%)]\t1.512794\n",
      "Train Epoch: 6 [44000/60000 (73%)]\t1.532750\n",
      "Train Epoch: 6 [46000/60000 (77%)]\t1.561870\n",
      "Train Epoch: 6 [48000/60000 (80%)]\t1.538858\n",
      "Train Epoch: 6 [50000/60000 (83%)]\t1.525102\n",
      "Train Epoch: 6 [52000/60000 (87%)]\t1.516387\n",
      "Train Epoch: 6 [54000/60000 (90%)]\t1.505987\n",
      "Train Epoch: 6 [56000/60000 (93%)]\t1.537359\n",
      "Train Epoch: 6 [58000/60000 (97%)]\t1.532045\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9669/10000 (97%\n",
      ")\n",
      "Train Epoch: 7 [0/60000 (0%)]\t1.530066\n",
      "Train Epoch: 7 [2000/60000 (3%)]\t1.572320\n",
      "Train Epoch: 7 [4000/60000 (7%)]\t1.596396\n",
      "Train Epoch: 7 [6000/60000 (10%)]\t1.509179\n",
      "Train Epoch: 7 [8000/60000 (13%)]\t1.494366\n",
      "Train Epoch: 7 [10000/60000 (17%)]\t1.536744\n",
      "Train Epoch: 7 [12000/60000 (20%)]\t1.517774\n",
      "Train Epoch: 7 [14000/60000 (23%)]\t1.527164\n",
      "Train Epoch: 7 [16000/60000 (27%)]\t1.525644\n",
      "Train Epoch: 7 [18000/60000 (30%)]\t1.553690\n",
      "Train Epoch: 7 [20000/60000 (33%)]\t1.547688\n",
      "Train Epoch: 7 [22000/60000 (37%)]\t1.544882\n",
      "Train Epoch: 7 [24000/60000 (40%)]\t1.511076\n",
      "Train Epoch: 7 [26000/60000 (43%)]\t1.527872\n",
      "Train Epoch: 7 [28000/60000 (47%)]\t1.511371\n",
      "Train Epoch: 7 [30000/60000 (50%)]\t1.555802\n",
      "Train Epoch: 7 [32000/60000 (53%)]\t1.563683\n",
      "Train Epoch: 7 [34000/60000 (57%)]\t1.521207\n",
      "Train Epoch: 7 [36000/60000 (60%)]\t1.516034\n",
      "Train Epoch: 7 [38000/60000 (63%)]\t1.544839\n",
      "Train Epoch: 7 [40000/60000 (67%)]\t1.527482\n",
      "Train Epoch: 7 [42000/60000 (70%)]\t1.552966\n",
      "Train Epoch: 7 [44000/60000 (73%)]\t1.592773\n",
      "Train Epoch: 7 [46000/60000 (77%)]\t1.592414\n",
      "Train Epoch: 7 [48000/60000 (80%)]\t1.577061\n",
      "Train Epoch: 7 [50000/60000 (83%)]\t1.538418\n",
      "Train Epoch: 7 [52000/60000 (87%)]\t1.555952\n",
      "Train Epoch: 7 [54000/60000 (90%)]\t1.522036\n",
      "Train Epoch: 7 [56000/60000 (93%)]\t1.524095\n",
      "Train Epoch: 7 [58000/60000 (97%)]\t1.556619\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9689/10000 (97%\n",
      ")\n",
      "Train Epoch: 8 [0/60000 (0%)]\t1.517474\n",
      "Train Epoch: 8 [2000/60000 (3%)]\t1.567575\n",
      "Train Epoch: 8 [4000/60000 (7%)]\t1.539768\n",
      "Train Epoch: 8 [6000/60000 (10%)]\t1.531741\n",
      "Train Epoch: 8 [8000/60000 (13%)]\t1.554623\n",
      "Train Epoch: 8 [10000/60000 (17%)]\t1.550719\n",
      "Train Epoch: 8 [12000/60000 (20%)]\t1.553684\n",
      "Train Epoch: 8 [14000/60000 (23%)]\t1.550046\n",
      "Train Epoch: 8 [16000/60000 (27%)]\t1.557610\n",
      "Train Epoch: 8 [18000/60000 (30%)]\t1.503875\n",
      "Train Epoch: 8 [20000/60000 (33%)]\t1.547621\n",
      "Train Epoch: 8 [22000/60000 (37%)]\t1.553969\n",
      "Train Epoch: 8 [24000/60000 (40%)]\t1.524960\n",
      "Train Epoch: 8 [26000/60000 (43%)]\t1.534421\n",
      "Train Epoch: 8 [28000/60000 (47%)]\t1.523944\n",
      "Train Epoch: 8 [30000/60000 (50%)]\t1.470281\n",
      "Train Epoch: 8 [32000/60000 (53%)]\t1.534579\n",
      "Train Epoch: 8 [34000/60000 (57%)]\t1.511123\n",
      "Train Epoch: 8 [36000/60000 (60%)]\t1.538266\n",
      "Train Epoch: 8 [38000/60000 (63%)]\t1.555071\n",
      "Train Epoch: 8 [40000/60000 (67%)]\t1.504425\n",
      "Train Epoch: 8 [42000/60000 (70%)]\t1.524199\n",
      "Train Epoch: 8 [44000/60000 (73%)]\t1.497010\n",
      "Train Epoch: 8 [46000/60000 (77%)]\t1.530642\n",
      "Train Epoch: 8 [48000/60000 (80%)]\t1.525897\n",
      "Train Epoch: 8 [50000/60000 (83%)]\t1.544718\n",
      "Train Epoch: 8 [52000/60000 (87%)]\t1.504388\n",
      "Train Epoch: 8 [54000/60000 (90%)]\t1.535661\n",
      "Train Epoch: 8 [56000/60000 (93%)]\t1.523059\n",
      "Train Epoch: 8 [58000/60000 (97%)]\t1.520123\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9707/10000 (97%\n",
      ")\n",
      "Train Epoch: 9 [0/60000 (0%)]\t1.522223\n",
      "Train Epoch: 9 [2000/60000 (3%)]\t1.581228\n",
      "Train Epoch: 9 [4000/60000 (7%)]\t1.520741\n",
      "Train Epoch: 9 [6000/60000 (10%)]\t1.534880\n",
      "Train Epoch: 9 [8000/60000 (13%)]\t1.487226\n",
      "Train Epoch: 9 [10000/60000 (17%)]\t1.517124\n",
      "Train Epoch: 9 [12000/60000 (20%)]\t1.541838\n",
      "Train Epoch: 9 [14000/60000 (23%)]\t1.505875\n",
      "Train Epoch: 9 [16000/60000 (27%)]\t1.499621\n",
      "Train Epoch: 9 [18000/60000 (30%)]\t1.542893\n",
      "Train Epoch: 9 [20000/60000 (33%)]\t1.575033\n",
      "Train Epoch: 9 [22000/60000 (37%)]\t1.532941\n",
      "Train Epoch: 9 [24000/60000 (40%)]\t1.510324\n",
      "Train Epoch: 9 [26000/60000 (43%)]\t1.510177\n",
      "Train Epoch: 9 [28000/60000 (47%)]\t1.500701\n",
      "Train Epoch: 9 [30000/60000 (50%)]\t1.507767\n",
      "Train Epoch: 9 [32000/60000 (53%)]\t1.555674\n",
      "Train Epoch: 9 [34000/60000 (57%)]\t1.518493\n",
      "Train Epoch: 9 [36000/60000 (60%)]\t1.485428\n",
      "Train Epoch: 9 [38000/60000 (63%)]\t1.568632\n",
      "Train Epoch: 9 [40000/60000 (67%)]\t1.525479\n",
      "Train Epoch: 9 [42000/60000 (70%)]\t1.531707\n",
      "Train Epoch: 9 [44000/60000 (73%)]\t1.519161\n",
      "Train Epoch: 9 [46000/60000 (77%)]\t1.529796\n",
      "Train Epoch: 9 [48000/60000 (80%)]\t1.525552\n",
      "Train Epoch: 9 [50000/60000 (83%)]\t1.569914\n",
      "Train Epoch: 9 [52000/60000 (87%)]\t1.554719\n",
      "Train Epoch: 9 [54000/60000 (90%)]\t1.515046\n",
      "Train Epoch: 9 [56000/60000 (93%)]\t1.506869\n",
      "Train Epoch: 9 [58000/60000 (97%)]\t1.519537\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9722/10000 (97%\n",
      ")\n",
      "Train Epoch: 10 [0/60000 (0%)]\t1.496408\n",
      "Train Epoch: 10 [2000/60000 (3%)]\t1.537613\n",
      "Train Epoch: 10 [4000/60000 (7%)]\t1.518563\n",
      "Train Epoch: 10 [6000/60000 (10%)]\t1.533749\n",
      "Train Epoch: 10 [8000/60000 (13%)]\t1.522465\n",
      "Train Epoch: 10 [10000/60000 (17%)]\t1.531994\n",
      "Train Epoch: 10 [12000/60000 (20%)]\t1.505209\n",
      "Train Epoch: 10 [14000/60000 (23%)]\t1.530631\n",
      "Train Epoch: 10 [16000/60000 (27%)]\t1.505506\n",
      "Train Epoch: 10 [18000/60000 (30%)]\t1.514741\n",
      "Train Epoch: 10 [20000/60000 (33%)]\t1.521405\n",
      "Train Epoch: 10 [22000/60000 (37%)]\t1.482514\n",
      "Train Epoch: 10 [24000/60000 (40%)]\t1.524278\n",
      "Train Epoch: 10 [26000/60000 (43%)]\t1.503792\n",
      "Train Epoch: 10 [28000/60000 (47%)]\t1.493028\n",
      "Train Epoch: 10 [30000/60000 (50%)]\t1.509295\n",
      "Train Epoch: 10 [32000/60000 (53%)]\t1.485545\n",
      "Train Epoch: 10 [34000/60000 (57%)]\t1.494776\n",
      "Train Epoch: 10 [36000/60000 (60%)]\t1.551974\n",
      "Train Epoch: 10 [38000/60000 (63%)]\t1.541903\n",
      "Train Epoch: 10 [40000/60000 (67%)]\t1.507719\n",
      "Train Epoch: 10 [42000/60000 (70%)]\t1.533607\n",
      "Train Epoch: 10 [44000/60000 (73%)]\t1.543428\n",
      "Train Epoch: 10 [46000/60000 (77%)]\t1.513039\n",
      "Train Epoch: 10 [48000/60000 (80%)]\t1.500234\n",
      "Train Epoch: 10 [50000/60000 (83%)]\t1.571350\n",
      "Train Epoch: 10 [52000/60000 (87%)]\t1.543113\n",
      "Train Epoch: 10 [54000/60000 (90%)]\t1.503504\n",
      "Train Epoch: 10 [56000/60000 (93%)]\t1.512446\n",
      "Train Epoch: 10 [58000/60000 (97%)]\t1.543707\n",
      "\n",
      "Test set: Average loss: 0.0149, Acuracy 9718/10000 (97%\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/p045mwns5lb5bvtrs9y0wsmr0000gn/T/ipykernel_381/3258896293.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[5]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim = 1, keepdim = True).item()\n",
    "\n",
    "print(f'Prediction: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model\n",
    "torch.save(model, './models/digitclassfier_model.pth')\n",
    "\n",
    "# Save only the model parameters (state_dict)\n",
    "torch.save(model.state_dict(), './models/model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire model\n",
    "full_model = torch.load('./models/digitclassfier_model.pth')\n",
    "\n",
    "# Load only model parameters\n",
    "loaded_state_model = CNN()\n",
    "loaded_state_model.load_state_dict(torch.load('./models/model_weights.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
